import torch
import torch.nn.functional as F
import json
import pickle
import numpy as np
from pathlib import Path
import os
import logging
from tqdm import tqdm
import torch.nn as nn
from torch.utils.data import Subset
import random
from torch.amp import autocast
import traceback
from typing import Dict

def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

class SwiGLU(torch.nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        # SwiGLUæ¿€æ´»å‡½æ•°ï¼šä½¿ç”¨ä¸¤ä¸ªçº¿æ€§å˜æ¢å’ŒSiLUé—¨æ§
        self.w1 = torch.nn.Linear(in_features, out_features, bias=True)
        self.w2 = torch.nn.Linear(in_features, out_features, bias=True)

    def forward(self, x):
        # SwiGLU(x) = W1(x) * SiLU(W2(x))
        return self.w1(x) * F.silu(self.w2(x))

class SENet(nn.Module):
    def __init__(self, num_features, reducation=4):
        super().__init__()
        self.se = nn.Sequential(
            nn.Linear(num_features, num_features // reducation),
            nn.ReLU(),
            nn.Linear(num_features // reducation, num_features),
            nn.Sigmoid()
        )
    
    def forward(self, x):  # [B, L, N, H]
        # åªåœ¨Hç»´åº¦ä¸Šæ± åŒ–ï¼Œä¿æŒæ›´å¤šä¿¡æ¯
        pooled = x.mean(dim=-1)  # [B, L, N]
        
        # é€šè¿‡SEæ¨¡å—è®¡ç®—ç‰¹å¾æƒé‡
        weights = self.se(pooled)  # [B, L, N]
        weights = weights.unsqueeze(-1)  # [B, L, N, 1]
        
        return x * weights

class BilinearInteraction(nn.Module):
    def __init__(self, num_fields, embedding_dim, selected_pairs=None):
        super().__init__()        
        self.W = nn.Parameter(torch.empty(embedding_dim, embedding_dim))
        nn.init.xavier_uniform_(self.W)
        
        if selected_pairs is None:
            # é»˜è®¤ï¼šå…¨éƒ¨äº¤å‰
            i_idx, j_idx = torch.triu_indices(num_fields, num_fields, offset=1)
        else:
            # åªå–ç”¨æˆ·æŒ‡å®šçš„äº¤å‰
            i_idx, j_idx = zip(*selected_pairs)
            i_idx = torch.tensor(i_idx, dtype=torch.long)
            j_idx = torch.tensor(j_idx, dtype=torch.long)
        # é¢„è®¡ç®—ç´¢å¼•ï¼ˆé¿å…æ¯æ¬¡ forward éƒ½è®¡ç®—ï¼‰
        self.register_buffer('i_idx', i_idx)
        self.register_buffer('j_idx', j_idx)

    def forward(self, x):  # x: [B, L, N, H]
        B, L, N, H = x.shape

        x_ = x.view(B*L, N, H)             # [B*L, N, H]
        Wx = x_ @ self.W                        # [B*L, N, H]

        vi = Wx[:, self.i_idx, :]               # [B*L, num_pairs, H]
        vj = x_[:, self.j_idx, :]               # [B*L, num_pairs, H]

        interactions = torch.einsum("bnh,bnh->bn", vi, vj).view(B, L, -1)

        return interactions

class SENetBilinearDNN(nn.Module):
    def __init__(self, input_dim, num_fields, hidden_size, dnn_hidden_units, 
                bilinear_weight=0.1, dropout_rate=0.1, selected_pairs=None):
        super().__init__()

        # è®¡ç®—æ€»è¾“å…¥ç»´åº¦
        self.input_dim = input_dim
        if selected_pairs is None:
            self.bilinear_dim = num_fields * (num_fields - 1) // 2
        else:
            self.bilinear_dim = len(selected_pairs)
        self.dropout_rate = dropout_rate

        self.se = SENet(num_fields)
        self.bilinear = BilinearInteraction(num_fields, hidden_size, selected_pairs)
        self.dnn = nn.Sequential(
            nn.Linear(self.input_dim + self.bilinear_dim, dnn_hidden_units),
            SwiGLU(dnn_hidden_units, dnn_hidden_units),
        )

        self.bilinear_weight = bilinear_weight
        # self.residual_proj = nn.Linear(self.input_dim, dnn_hidden_units)
        # self.norm = nn.RMSNorm(dnn_hidden_units)

    def forward(self, x):  # [B, L, N, H]
        # Step 1: SENet é‡æ ‡å®š
        x = self.se(x)  # [B, L, N, H]
        
        # Step 2: Bilinear äº¤å‰
        cross_feats = self.bilinear(x)  # [B, L, N*(N-1)/2]
        if self.dropout_rate > 0:
            cross_feats = F.dropout(cross_feats, self.dropout_rate, training=self.training)
        cross_feats = cross_feats * self.bilinear_weight  # å‡å¼±Bilinearå½±å“
        
        # Step 3: æ‹¼æ¥ + DNN
        x_flat = x.view(x.size(0), x.size(1), -1).contiguous()  # [B, L, N*H]
        combined = torch.cat([x_flat, cross_feats], dim=-1)  # [B, L, N*H + N*(N-1)/2]
        output = self.dnn(combined)  # [B, L, D]

        # Step 4: æ®‹å·®è¿æ¥
        # output = self.norm(output + self.residual_proj(x_flat))  # [B, L, D]

        return output


def optimized_embedding_init(model, args):
    """ä¼˜åŒ–çš„embeddingåˆå§‹åŒ–ç­–ç•¥"""
    
    # 1. æ ¸å¿ƒID embedding
    item_std_core = np.sqrt(2.0 / args.hidden_units)
    user_std_core = np.sqrt(2.0 / args.hidden_units)
    torch.nn.init.normal_(model.item_emb.weight.data, std=item_std_core)
    torch.nn.init.normal_(model.user_emb.weight.data, std=user_std_core)
    
    # 2. ä½ç½®embedding - ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
    # torch.nn.init.normal_(model.pos_emb.weight.data, std=0.02)
    
    # 3. ç¨€ç–ç‰¹å¾embedding - æ”¹è¿›çš„é¢‘ç‡æ„ŸçŸ¥åˆå§‹åŒ–
    for k, emb_layer in model.sparse_emb.items():
        vocab_size = emb_layer.num_embeddings
        
        # ğŸ”§ æ”¹è¿›çš„åˆå§‹åŒ–ç­–ç•¥ï¼šé¿å…å°è¯æ±‡è¡¨æ ‡å‡†å·®è¿‡å°
        if vocab_size <= 10:  # å°è¯æ±‡è¡¨ç‰¹å¾
            std = 0.05  # å›ºå®šä½¿ç”¨è¾ƒå¤§çš„æ ‡å‡†å·®ï¼Œä¿è¯è¡¨è¾¾èƒ½åŠ›
        elif vocab_size <= 100:  # ä¸­ç­‰è¯æ±‡è¡¨ç‰¹å¾
            std = 0.08
        else:  # å¤§è¯æ±‡è¡¨ç‰¹å¾
            std = min(0.1, np.sqrt(2.0 / (vocab_size + args.hidden_units)))
            
        # ç¡®ä¿æœ€å°æ ‡å‡†å·®ä¸å°äº0.02
        std = max(std, 0.02)
        
        torch.nn.init.normal_(emb_layer.weight.data, std=std)
    
    # 4. å¤šæ¨¡æ€ç‰¹å¾å˜æ¢å±‚
    for transform_layer in model.emb_transform.values():
        torch.nn.init.xavier_uniform_(transform_layer.weight.data, gain=0.1)
        if transform_layer.bias is not None:
            torch.nn.init.zeros_(transform_layer.bias.data)
    
    # 5. Padding tokenå¤„ç†
    # model.pos_emb.weight.data[0, :] = 0
    model.item_emb.weight.data[0, :] = 0  
    model.user_emb.weight.data[0, :] = 0
    for emb_layer in model.sparse_emb.values():
        emb_layer.weight.data[0, :] = 0


def validate_initialization(model):
    """éªŒè¯åˆå§‹åŒ–æ˜¯å¦åˆç†"""
    for name, param in model.named_parameters():
        if 'weight' in name and param.dim() >= 2:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            std = param.std().item()
            mean = param.mean().item()
            print(f"{name}: mean={mean:.6f}, std={std:.6f}")
            
            # è­¦å‘Šå¼‚å¸¸æƒ…å†µ
            if std > 1.0 or std < 1e-4:
                print(f"Warning: {name} has unusual std: {std}")


def split_dataset(dataset, train_ratio=0.9, seed=42):
    rng = np.random.default_rng(seed)
    indices = np.arange(len(dataset))
    rng.shuffle(indices)
    split = int(len(indices) * train_ratio)
    train_idx, valid_idx = indices[:split], indices[split:]
    return train_idx, valid_idx

class CachedComplexRoPE(nn.Module):
    """é«˜ç²¾åº¦ç¼“å­˜å¤æ•°æ—‹è½¬å› å­çš„RoPEå®ç°"""
    
    def __init__(self, head_dim, max_seq_len=8192, base=10000.0):
        """
        Args:
            head_dim: å¤´ç»´åº¦
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            base: ä½ç½®ç¼–ç åŸºæ•°
        """
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.base = base
        
        # æ ¹æ®ç²¾åº¦ç­‰çº§è®¾ç½®è®¡ç®—ç²¾åº¦
        self.compute_dtype = torch.float64
        
        # é¢„è®¡ç®—å¹¶ç¼“å­˜é«˜ç²¾åº¦å¤æ•°æ—‹è½¬å› å­
        self._precompute_complex_rotations()
    
    def _precompute_complex_rotations(self):
        """é¢„è®¡ç®—é«˜ç²¾åº¦å¤æ•°æ—‹è½¬å› å­"""
        assert self.head_dim % 2 == 0
        
        # ä½¿ç”¨é«˜ç²¾åº¦è®¡ç®—
        device = torch.device('cpu')  # åœ¨CPUä¸Šé¢„è®¡ç®—ä»¥é¿å…GPUå†…å­˜é—®é¢˜
        
        # è®¡ç®—é¢‘ç‡ (ä½¿ç”¨æ›´é«˜ç²¾åº¦)
        freq_indices = torch.arange(0, self.head_dim, 2, dtype=self.compute_dtype)
        freqs = 1.0 / (self.base ** (freq_indices / self.head_dim))
        
        positions = torch.arange(self.max_seq_len, dtype=self.compute_dtype)
        angles = torch.outer(positions, freqs)  # [max_seq_len, head_dim//2]
        
        # é¢„è®¡ç®—å¤æ•°æ—‹è½¬å› å­ e^(i*angles) - ä½¿ç”¨æ›´é«˜ç²¾åº¦
        ones = torch.ones_like(angles)
        rotate_complex = torch.polar(ones, angles)
        
        # æ³¨å†Œä¸ºbufferï¼Œä½†ä¿æŒé«˜ç²¾åº¦
        self.register_buffer('rotate_complex_cached_high', rotate_complex, persistent=False)
    
    def forward(self, x, seq_dim=-2):
        """é«˜ç²¾åº¦å¤æ•°RoPEå‰å‘ä¼ æ’­"""
        seq_len = x.size(seq_dim)
        device = x.device
        x = x.to(torch.float32)
        original_dtype = x.dtype
        
        # è·å–ç¼“å­˜çš„é«˜ç²¾åº¦æ—‹è½¬å› å­å¹¶è½¬ç§»åˆ°è¾“å…¥è®¾å¤‡
        rotate_complex = self.rotate_complex_cached_high[:seq_len].to(device)  # [seq_len, head_dim//2]
        
        rotate_complex = rotate_complex.to(torch.complex64)
        
        # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…è¾“å…¥å¼ é‡
        while rotate_complex.dim() < x.dim():
            rotate_complex = rotate_complex.unsqueeze(0)
        
        # å°†è¾“å…¥è½¬ä¸ºå¤æ•°å½¢å¼
        x_reshaped = x.view(*x.shape[:-1], self.head_dim // 2, 2)
        x_complex = torch.view_as_complex(x_reshaped)
        
        # å¤æ•°æ—‹è½¬ï¼ˆä¿æŒç²¾åº¦ï¼‰
        with autocast(device_type="cuda", enabled=False):  # â† ç¦ç”¨ autocastï¼Œé¿å… complex32
            rotated_complex = x_complex * rotate_complex  # å®‰å…¨åœ°ç”¨ complex64 è®¡ç®—
        
        # è½¬å›å®æ•°
        result = torch.view_as_real(rotated_complex).view_as(x)
        
        return result.to(original_dtype)