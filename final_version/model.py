from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
from utils import SwiGLU, SENet, CachedComplexRoPE, SENetBilinearDNN


from dataset import save_emb


class FlashMultiHeadAttention(torch.nn.Module):
    def __init__(self, hidden_units, num_heads, dropout_rate):
        super(FlashMultiHeadAttention, self).__init__()

        self.hidden_units = hidden_units
        self.num_heads = num_heads
        self.head_dim = hidden_units // num_heads
        # self.dropout_rate = dropout_rate
        self.dropout_rate = 0.0

        assert hidden_units % num_heads == 0, "hidden_units must be divisible by num_heads"

        self.qkv_linear = torch.nn.Linear(hidden_units, hidden_units * 3)
        self.out_linear = torch.nn.Linear(hidden_units, hidden_units)

        self.rope = CachedComplexRoPE(head_dim=self.head_dim, max_seq_len=102, base=10000.0)
    
        
    def forward(self, query, key, value, attn_mask=None):
        batch_size, seq_len, _ = query.size()

        # è®¡ç®—Q, K, V
        qkv = self.qkv_linear(query)
        Q, K, V = qkv.chunk(3, dim=-1)


        # reshapeä¸ºmulti-headæ ¼å¼
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # RoPE
        Q = self.rope(Q, seq_dim=-2)
        K = self.rope(K, seq_dim=-2)

        if hasattr(F, 'scaled_dot_product_attention'):
            # PyTorch 2.0+ ä½¿ç”¨å†…ç½®çš„Flash Attention
            attn_output = F.scaled_dot_product_attention(
                Q, K, V, dropout_p=self.dropout_rate if self.training else 0.0, attn_mask=attn_mask.unsqueeze(1)
            )
        else:
            # é™çº§åˆ°æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶
            scale = (self.head_dim) ** -0.5
            scores = torch.matmul(Q, K.transpose(-2, -1)) * scale

            if attn_mask is not None:
                scores.masked_fill_(attn_mask.unsqueeze(1).logical_not(), float('-inf'))

            attn_weights = F.softmax(scores, dim=-1)
            attn_weights = F.dropout(attn_weights, p=self.dropout_rate, training=self.training)
            attn_output = torch.matmul(attn_weights, V)

        # reshapeå›åŸæ¥çš„æ ¼å¼
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_units)
        # æœ€ç»ˆçš„çº¿æ€§å˜æ¢å¹¶æ·»åŠ LayerNormï¼Œåœ¨å¤–éƒ¨å·²ç»åšäº†ï¼Œæ‰€ä»¥åç»­è¿™é‡Œå¯ä»¥å»æ‰
        output = self.out_linear(attn_output)

        return output, None


class PointWiseFeedForward(torch.nn.Module):
    def __init__(self, hidden_units, dropout_rate):
        super(PointWiseFeedForward, self).__init__()

        self.liner1 = torch.nn.Linear(hidden_units, hidden_units * 4)
        self.activation = SwiGLU(hidden_units * 4, hidden_units * 4)
        # self.dropout1 = torch.nn.Dropout(p=dropout_rate)
        self.dropout1 = torch.nn.Dropout(p=0.0)

        self.liner2 = torch.nn.Linear(hidden_units * 4, hidden_units)
        # self.dropout2 = torch.nn.Dropout(p=dropout_rate)
        self.dropout2 = torch.nn.Dropout(p=0.0)
        self.norm = torch.nn.RMSNorm(hidden_units, eps=1e-8)

    def forward(self, inputs):
        residual = inputs
        outputs = self.dropout2(self.liner2(self.dropout1(self.activation(self.liner1(inputs)))))
        outputs = self.norm(outputs + residual)
        return outputs


class BaselineModel(torch.nn.Module):
    """
    Args:
        user_num: ç”¨æˆ·æ•°é‡
        item_num: ç‰©å“æ•°é‡
        feat_statistics: ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾æ•°é‡
        feat_types: å„ä¸ªç‰¹å¾çš„ç‰¹å¾ç±»å‹ï¼Œkeyä¸ºç‰¹å¾ç±»å‹åç§°ï¼Œvalueä¸ºåŒ…å«çš„ç‰¹å¾IDåˆ—è¡¨ï¼ŒåŒ…æ‹¬userå’Œitemçš„sparse, array, emb, continualç±»å‹
        args: å…¨å±€å‚æ•°

    Attributes:
        user_num: ç”¨æˆ·æ•°é‡
        item_num: ç‰©å“æ•°é‡
        dev: è®¾å¤‡
        norm_first: æ˜¯å¦å…ˆå½’ä¸€åŒ–
        maxlen: åºåˆ—æœ€å¤§é•¿åº¦
        item_emb: Item Embedding Table
        user_emb: User Embedding Table
        sparse_emb: ç¨€ç–ç‰¹å¾Embedding Table
        emb_transform: å¤šæ¨¡æ€ç‰¹å¾çš„çº¿æ€§å˜æ¢
        userdnn: ç”¨æˆ·ç‰¹å¾æ‹¼æ¥åç»è¿‡çš„å…¨è¿æ¥å±‚
        itemdnn: ç‰©å“ç‰¹å¾æ‹¼æ¥åç»è¿‡çš„å…¨è¿æ¥å±‚
    """

    def __init__(self, user_num, item_num, feat_statistics, feat_types, args, writer):  #
        super(BaselineModel, self).__init__()

        self.user_num = user_num
        self.item_num = item_num
        self.dev = args.device
        self.norm_first = args.norm_first
        self.maxlen = args.maxlen
        self.temperature = args.temperature
        self.writer = writer
        
        # Flash InfoNCEé…ç½® - ä¿æŒåŸæœ‰
        self.use_flash_infonce = getattr(args, 'use_flash_infonce', True)
        self.flash_block_size = getattr(args, 'flash_block_size', 1024)
        
        # TODO: loss += args.l2_emb for regularizing embedding vectors during training
        # https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch

        self.item_emb = torch.nn.Embedding(self.item_num + 1, args.hidden_units, padding_idx=0)
        self.user_emb = torch.nn.Embedding(self.user_num + 1, args.hidden_units, padding_idx=0)
        # self.pos_emb = torch.nn.Embedding(2 * args.maxlen + 1, args.hidden_units, padding_idx=0)

        # self.emb_dropout = torch.nn.Dropout(p=args.dropout_rate)
        self.sparse_emb = torch.nn.ModuleDict()
        self.emb_transform = torch.nn.ModuleDict()

        # SEæ³¨æ„åŠ›æœºåˆ¶
        self.continual_transform = torch.nn.ModuleDict()

        self.attention_layernorms = torch.nn.ModuleList()  # to be Q for self-attention
        self.attention_layers = torch.nn.ModuleList()
        self.forward_layernorms = torch.nn.ModuleList()
        self.forward_layers = torch.nn.ModuleList()

        self._init_feat_info(feat_statistics, feat_types)

        self.user_feat_num = len(self.USER_SPARSE_FEAT) + 1 + len(self.USER_ARRAY_FEAT) + len(self.USER_CONTINUAL_FEAT)
        self.item_feat_num = len(self.ITEM_SPARSE_FEAT) + 1 + len(self.ITEM_ARRAY_FEAT) + len(self.ITEM_CONTINUAL_FEAT) + len(self.ITEM_EMB_FEAT)

        # ç‰¹å¾åŠ æƒ + ç‰¹å¾äº¤å‰ + DNN
        self.userSeBilinearDNN = SENetBilinearDNN(
            self.user_feat_num * args.hidden_units,
            self.user_feat_num,
            args.hidden_units,
            args.hidden_units,
            bilinear_weight=0.1,
            # dropout_rate=args.dropout_rate
            dropout_rate=0.0,
            selected_pairs=((0, 1), (0, 6), (0, 7), (6, 7))
        )
        self.itemSeBilinearDNN = SENetBilinearDNN(
            self.item_feat_num * args.hidden_units,
            self.item_feat_num,
            args.hidden_units,
            args.hidden_units,
            bilinear_weight=0.1,
            # dropout_rate=args.dropout_rate
            dropout_rate=0.0,
            selected_pairs=((0, 3), (0, 11), (0, 15), (3, 11))
        )
        
        self.last_layernorm = torch.nn.RMSNorm(args.hidden_units, eps=1e-8)

        for _ in range(args.num_blocks):
            new_attn_layernorm = torch.nn.RMSNorm(args.hidden_units, eps=1e-8)
            self.attention_layernorms.append(new_attn_layernorm)

            new_attn_layer = FlashMultiHeadAttention(
                args.hidden_units, args.num_heads, args.dropout_rate
            )  # ä¼˜åŒ–ï¼šç”¨FlashAttentionæ›¿ä»£æ ‡å‡†Attention
            self.attention_layers.append(new_attn_layer)

            new_fwd_layernorm = torch.nn.RMSNorm(args.hidden_units, eps=1e-8)
            self.forward_layernorms.append(new_fwd_layernorm)

            new_fwd_layer = PointWiseFeedForward(args.hidden_units, args.dropout_rate)
            self.forward_layers.append(new_fwd_layer)

        for k in self.USER_SPARSE_FEAT:
            if k in ['301', '302']:
                self.sparse_emb[k] = torch.nn.Embedding(self.USER_SPARSE_FEAT[k], args.hidden_units)
            else:
                self.sparse_emb[k] = torch.nn.Embedding(self.USER_SPARSE_FEAT[k] + 1, args.hidden_units, padding_idx=0)
        for k in self.ITEM_SPARSE_FEAT:
            self.sparse_emb[k] = torch.nn.Embedding(self.ITEM_SPARSE_FEAT[k] + 1, args.hidden_units, padding_idx=0)
        for k in self.ITEM_ARRAY_FEAT:
            self.sparse_emb[k] = torch.nn.Embedding(self.ITEM_ARRAY_FEAT[k] + 1, args.hidden_units, padding_idx=0)
        for k in self.USER_ARRAY_FEAT:
            self.sparse_emb[k] = torch.nn.Embedding(self.USER_ARRAY_FEAT[k] + 1, args.hidden_units, padding_idx=0)
        # ä¸ºå¤šæ¨¡æ€ç‰¹å¾å˜æ¢æ·»åŠ LayerNorm,åµŒå…¥å±‚åä¸åº”è¯¥ç«‹å³è·Ÿlayernorm
        for k in self.ITEM_EMB_FEAT:
            self.emb_transform[k] = torch.nn.Linear(self.ITEM_EMB_FEAT[k], args.hidden_units)

        # ä¸ºè¿ç»­ç‰¹å¾æ·»åŠ å˜æ¢å±‚ï¼Œå°†1ç»´æ˜ å°„åˆ°hidden_unitsç»´
        for k in self.ITEM_CONTINUAL_FEAT:
            self.continual_transform[k] = torch.nn.Sequential(
                torch.nn.Linear(1, args.hidden_units),
                torch.nn.RMSNorm(args.hidden_units, eps=1e-8),
                SwiGLU(args.hidden_units, args.hidden_units)
            )
        for k in self.USER_CONTINUAL_FEAT:
            self.continual_transform[k] = torch.nn.Sequential(
                torch.nn.Linear(1, args.hidden_units),
                torch.nn.RMSNorm(args.hidden_units, eps=1e-8),
                SwiGLU(args.hidden_units, args.hidden_units)
            )

    def _init_feat_info(self, feat_statistics, feat_types):
        """
        å°†ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼ˆç‰¹å¾æ•°é‡ï¼‰æŒ‰ç‰¹å¾ç±»å‹åˆ†ç»„äº§ç”Ÿä¸åŒçš„å­—å…¸ï¼Œæ–¹ä¾¿å£°æ˜ç¨€ç–ç‰¹å¾çš„Embedding Table

        Args:
            feat_statistics: ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾æ•°é‡
            feat_types: å„ä¸ªç‰¹å¾çš„ç‰¹å¾ç±»å‹ï¼Œkeyä¸ºç‰¹å¾ç±»å‹åç§°ï¼Œvalueä¸ºåŒ…å«çš„ç‰¹å¾IDåˆ—è¡¨ï¼ŒåŒ…æ‹¬userå’Œitemçš„sparse, array, emb, continualç±»å‹
        """
        self.USER_SPARSE_FEAT = {k: feat_statistics[k] for k in feat_types['user_sparse']}
        self.USER_CONTINUAL_FEAT = feat_types['user_continual']
        self.ITEM_SPARSE_FEAT = {k: feat_statistics[k] for k in feat_types['item_sparse']}
        self.ITEM_CONTINUAL_FEAT = feat_types['item_continual']
        self.USER_ARRAY_FEAT = {k: feat_statistics[k] for k in feat_types['user_array']}
        self.ITEM_ARRAY_FEAT = {k: feat_statistics[k] for k in feat_types['item_array']}
        EMB_SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
        self.ITEM_EMB_FEAT = {k: EMB_SHAPE_DICT[k] for k in feat_types['item_emb']}  # è®°å½•çš„æ˜¯ä¸åŒå¤šæ¨¡æ€ç‰¹å¾çš„ç»´åº¦



    def feat2emb(self, seq, feature_tensors, mask=None, include_user=False):
        """
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„ç‰¹å¾tensorï¼Œé¿å…é‡å¤è½¬æ¢
        
        Args:
            seq: åºåˆ—ID
            feature_tensors: é¢„å¤„ç†å¥½çš„ç‰¹å¾tensorå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºtensor
            mask: æ©ç ï¼Œ1è¡¨ç¤ºitemï¼Œ2è¡¨ç¤ºuser
            include_user: æ˜¯å¦å¤„ç†ç”¨æˆ·ç‰¹å¾

        Returns:
            seqs_emb: åºåˆ—ç‰¹å¾çš„Embedding
        """
        seq = seq.to(self.dev)
        

        if include_user:
            user_mask = (mask == 2).to(self.dev)
            item_mask = (mask == 1).to(self.dev)
            user_embedding = self.user_emb(user_mask * seq)
            item_embedding = self.item_emb(item_mask * seq)
            item_feat_list = [item_embedding]
            user_feat_list = [user_embedding]
        else:
            item_embedding = self.item_emb(seq)
            item_feat_list = [item_embedding]


        all_feat_types = [
            (self.ITEM_SPARSE_FEAT, 'item_sparse', item_feat_list),
            (self.ITEM_ARRAY_FEAT, 'item_array', item_feat_list),
            (self.ITEM_CONTINUAL_FEAT, 'item_continual', item_feat_list),
        ]

        if include_user:
            all_feat_types.extend([
                (self.USER_SPARSE_FEAT, 'user_sparse', user_feat_list),
                (self.USER_ARRAY_FEAT, 'user_array', user_feat_list),
                (self.USER_CONTINUAL_FEAT, 'user_continual', user_feat_list),
            ])

        # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„tensorè¿›è¡Œembedding
        for feat_dict, feat_type, feat_list in all_feat_types:
            if not feat_dict:
                continue

            for k in feat_dict:
                if k not in feature_tensors:
                    continue
                    
                tensor_feature = feature_tensors[k].to(self.dev)

                if feat_type.endswith('sparse'):
                    feat_list.append(self.sparse_emb[k](tensor_feature))
                elif feat_type.endswith('array'):
                    feat_list.append(self.sparse_emb[k](tensor_feature).sum(2))
                elif feat_type.endswith('continual'):
                    # feat_list.append(tensor_feature)
                    feat_list.append(self.continual_transform[k](tensor_feature))

                del tensor_feature


        # åˆ†å—å¤„ç†å¤šæ¨¡æ€åµŒå…¥ç‰¹å¾ - æ›´æ¿€è¿›çš„å†…å­˜ç®¡ç†
        for k in self.ITEM_EMB_FEAT:
            if k in feature_tensors:  
                # è·å–CPUä¸Šçš„tensorä¿¡æ¯ï¼Œä½†ä¸ç«‹å³è½¬ç§»åˆ°GPU
                tensor_feature_cpu = feature_tensors[k]
                batch_size, seq_len, emb_dim = tensor_feature_cpu.shape
                # print(f"    ğŸ”§ Processing multimodal {k}: {tensor_feature_cpu.shape} = {tensor_feature_cpu.numel()*4/(1024**3):.3f}GB")
                
                # æå°çš„chunk_sizeï¼Œé€å—è½¬ç§»å’Œå¤„ç†
                chunk_size = 8  # ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªä½ç½®
                transformed_chunks = []
                
                for start_idx in range(0, seq_len, chunk_size):
                    end_idx = min(start_idx + chunk_size, seq_len)
                    
                    # åªè½¬ç§»å½“å‰éœ€è¦çš„å°å—åˆ°GPU
                    chunk_cpu = tensor_feature_cpu[:, start_idx:end_idx, :]
                    chunk_gpu = chunk_cpu.to(self.dev)
                    
                    try:
                        with torch.no_grad():
                            transformed_chunk = self.emb_transform[k](chunk_gpu)
                        transformed_chunks.append(transformed_chunk)
                    except RuntimeError as e:
                        print(f"    âŒ Chunk {start_idx}-{end_idx} failed: {e}")
                        # å¤±è´¥æ—¶åˆ›å»ºé›¶å¼ é‡
                        zero_chunk = torch.zeros(
                            batch_size, end_idx - start_idx, self.emb_transform[k].out_features,
                            device=self.dev, dtype=chunk_gpu.dtype
                        )
                        transformed_chunks.append(zero_chunk)
                    
                    # ç«‹å³é‡Šæ”¾chunk
                    del chunk_cpu, chunk_gpu
                    
                    # æ¯ä¸ªchunkéƒ½æ¸…ç†æ˜¾å­˜
                    torch.cuda.empty_cache()
                
                if transformed_chunks:
                    transformed_emb = torch.cat(transformed_chunks, dim=1)
                    item_feat_list.append(transformed_emb)
                    del transformed_chunks, transformed_emb
                
                torch.cuda.empty_cache()

        # for k in self.ITEM_EMB_FEAT:
        #     if k in feature_tensors:
        #         tensor_feature = feature_tensors[k].to(self.dev)
        #         transformed_emb = self.emb_transform[k](tensor_feature)
        #         item_feat_list.append(transformed_emb)

        all_item_emb = self.itemSeBilinearDNN(torch.stack(item_feat_list, dim=2))
        # all_item_emb = 0
        #small_chunk=8
        #for i in range(0, len(item_feat_list), small_chunk):
        #    chunk_emb = torch.stack(item_feat_list[i:i+small_chunk], dim=2).to(self.dev)
        #    all_item_emb += self.itemSeBilinearDNN(chunk_emb)
        #    del chunk_emb
        #    torch.cuda.empty_cache()
        # all_item_emb = transformed_sum
        
        if include_user:
            # SEæ³¨æ„åŠ›
            all_user_emb = self.userSeBilinearDNN(torch.stack(user_feat_list, dim=2))
            #all_user_emb = 0
            #small_chunk=8
            #for i in range(0, len(item_feat_list), small_chunk):
            #    chunk_emb = torch.stack(item_feat_list[i:i+small_chunk], dim=2).to(self.dev)
            #    all_user_emb += self.itemSeBilinearDNN(chunk_emb)
            #    del chunk_emb
            #    torch.cuda.empty_cache()
            seqs_emb = all_item_emb + all_user_emb
        else:
            seqs_emb = all_item_emb
            
        return seqs_emb

    def log2feats(self, log_seqs, mask, seq_feature_tensors):
        """
        Args:
            log_seqs: åºåˆ—ID
            mask: tokenç±»å‹æ©ç ï¼Œ1è¡¨ç¤ºitem tokenï¼Œ2è¡¨ç¤ºuser token
            seq_feature_tensors: é¢„å¤„ç†å¥½çš„åºåˆ—ç‰¹å¾tensorå­—å…¸

        Returns:
            seqs_emb: åºåˆ—çš„Embeddingï¼Œå½¢çŠ¶ä¸º [batch_size, maxlen, hidden_units]
        """
        batch_size = log_seqs.shape[0]
        maxlen = log_seqs.shape[1]
        seqs = self.feat2emb(log_seqs, seq_feature_tensors, mask=mask, include_user=True)
        seqs *= self.item_emb.embedding_dim**0.5

        maxlen = seqs.shape[1]
        ones_matrix = torch.ones((maxlen, maxlen), dtype=torch.bool, device=self.dev)
        attention_mask_tril = torch.tril(ones_matrix)
        attention_mask_pad = (mask != 0).to(self.dev)
        attention_mask = attention_mask_tril.unsqueeze(0) & attention_mask_pad.unsqueeze(1)

        for i in range(len(self.attention_layers)):
            if self.norm_first:
                # Pre-Normæ¶æ„ï¼šå…ˆå½’ä¸€åŒ–å†å¤„ç†
                # 1. è‡ªæ³¨æ„åŠ›
                x = self.attention_layernorms[i](seqs)
                mha_outputs, _ = self.attention_layers[i](x, x, x, attn_mask=attention_mask)
                seqs = seqs + mha_outputs
                seqs = seqs + self.forward_layers[i](self.forward_layernorms[i](seqs))
            else:
                # Post-Normæ¶æ„ï¼šå…ˆå¤„ç†å†å½’ä¸€åŒ–
                # 1. è‡ªæ³¨æ„åŠ›
                mha_outputs, _ = self.attention_layers[i](seqs, seqs, seqs, attn_mask=attention_mask)
                seqs = self.attention_layernorms[i](seqs + mha_outputs)
                seqs = self.forward_layernorms[i](seqs + self.forward_layers[i](seqs))

        log_feats = self.last_layernorm(seqs)

        return log_feats
    
    def predict(self, log_seqs, seq_feature_tensors, mask):
        """
        è®¡ç®—ç”¨æˆ·åºåˆ—çš„è¡¨å¾
        Args:
            log_seqs: ç”¨æˆ·åºåˆ—ID
            seq_feature_tensors: é¢„å¤„ç†å¥½çš„åºåˆ—ç‰¹å¾tensorå­—å…¸
            mask: tokenç±»å‹æ©ç ï¼Œ1è¡¨ç¤ºitem tokenï¼Œ2è¡¨ç¤ºuser token
        Returns:
            final_feat: ç”¨æˆ·åºåˆ—çš„è¡¨å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, hidden_units]
        """
        log_feats = self.log2feats(log_seqs, mask, seq_feature_tensors)

        final_feat = log_feats[:, -1, :]
        
        # æ¨ç†æ—¶ä¹Ÿè¿›è¡ŒL2å½’ä¸€åŒ–ï¼Œä¸è®­ç»ƒä¿æŒä¸€è‡´
        final_feat = F.normalize(final_feat, dim=-1)

        return final_feat

    def save_item_emb(self, item_ids, retrieval_ids, feat_dict, save_path, batch_size=1024):
        """
        ç”Ÿæˆå€™é€‰åº“item embeddingï¼Œç”¨äºæ£€ç´¢

        Args:
            item_ids: å€™é€‰item IDï¼ˆre-idå½¢å¼ï¼‰
            retrieval_ids: å€™é€‰item IDï¼ˆæ£€ç´¢IDï¼Œä»0å¼€å§‹ç¼–å·ï¼Œæ£€ç´¢è„šæœ¬ä½¿ç”¨ï¼‰
            feat_dict: è®­ç»ƒé›†æ‰€æœ‰itemç‰¹å¾å­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾å€¼
            save_path: ä¿å­˜è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        all_embs = []

        for start_idx in tqdm(range(0, len(item_ids), batch_size), desc="Saving item embeddings"):
            end_idx = min(start_idx + batch_size, len(item_ids))

            item_seq = torch.tensor(item_ids[start_idx:end_idx], device=self.dev).unsqueeze(0)
            batch_feat = []
            for i in range(start_idx, end_idx):
                batch_feat.append(feat_dict[i])
            # print(f"batch_feat :{batch_feat}")

            # éœ€è¦åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç‰¹å¾å¤„ç†å™¨æ¥è½¬æ¢ç‰¹å¾
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿdatasetçš„ç‰¹å¾é¢„å¤„ç†è¿‡ç¨‹
            feature_batch = [batch_feat]  # åŒ…è£…æˆbatchæ ¼å¼
            # print(f"feature_batch : {len(feature_batch[0])}")
            feature_tensors = self._preprocess_item_features_for_inference(feature_batch, len(item_ids[start_idx:end_idx]))


            batch_emb = self.feat2emb(item_seq, feature_tensors, include_user=False).squeeze(0)
            # batch_emb = batch_emb / batch_emb.norm(dim=-1, keepdim=True)
            batch_emb = F.normalize(batch_emb, dim=-1)

            all_embs.append(batch_emb.detach().cpu().numpy().astype(np.float32))

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœå¹¶ä¿å­˜
        final_ids = np.array(retrieval_ids, dtype=np.uint64).reshape(-1, 1)
        final_embs = np.concatenate(all_embs, axis=0)
        save_emb(final_embs, Path(save_path, 'embedding.fbin'))
        save_emb(final_ids, Path(save_path, 'id.u64bin'))

    def _preprocess_item_features_for_inference(self, feature_batch, seq_len):
        """
        ä¸ºæ¨ç†é˜¶æ®µé¢„å¤„ç†ç‰©å“ç‰¹å¾ï¼Œç®€åŒ–ç‰ˆæœ¬çš„ç‰¹å¾è½¬æ¢
        
        Args:
            feature_batch: ç‰¹å¾batchï¼Œå½¢çŠ¶ä¸º [1, seq_len]
            seq_len: åºåˆ—é•¿åº¦
            
        Returns:
            feature_tensors: é¢„å¤„ç†å¥½çš„ç‰¹å¾tensorå­—å…¸
        """
        batch_size = 1  # æ¨ç†æ—¶batch_sizeå›ºå®šä¸º1
        feature_tensors = {}
        
        # å¤„ç†ç‰©å“ç‰¹å¾
        for feat_id in self.ITEM_SPARSE_FEAT:
            batch_data = np.zeros((batch_size, seq_len), dtype=np.int64)
            for j, item_feat in enumerate(feature_batch[0]):
                batch_data[0, j] = item_feat.get(feat_id, 0)
            feature_tensors[feat_id] = torch.from_numpy(batch_data)

        # print(feature_tensors)
        
        for feat_id in self.ITEM_ARRAY_FEAT:
            max_array_len = 1
            # è®¡ç®—æœ€å¤§æ•°ç»„é•¿åº¦
            for item_feat in feature_batch[0]:
                if feat_id in item_feat and isinstance(item_feat[feat_id], list):
                    max_array_len = max(max_array_len, len(item_feat[feat_id]))
            
            batch_data = np.zeros((batch_size, seq_len, max_array_len), dtype=np.int64)
            for j, item_feat in enumerate(feature_batch[0]):
                if feat_id in item_feat:
                    item_data = item_feat[feat_id]
                    if isinstance(item_data, list):
                        actual_len = min(len(item_data), max_array_len)
                        batch_data[0, j, :actual_len] = item_data[:actual_len]
                    else:
                        batch_data[0, j, 0] = item_data
            feature_tensors[feat_id] = torch.from_numpy(batch_data)
        
        for feat_id in self.ITEM_CONTINUAL_FEAT:
            batch_data = np.zeros((batch_size, seq_len, 1), dtype=np.float32)
            for j, item_feat in enumerate(feature_batch[0]):
                batch_data[0, j, 0] = float(item_feat.get(feat_id, 0))
            feature_tensors[feat_id] = torch.from_numpy(batch_data)
        
        for feat_id in self.ITEM_EMB_FEAT:
            emb_dim = self.ITEM_EMB_FEAT[feat_id]
            batch_data = np.zeros((batch_size, seq_len, emb_dim), dtype=np.float32)
            for j, item_feat in enumerate(feature_batch[0]):
                if feat_id in item_feat and isinstance(item_feat[feat_id], np.ndarray):
                    batch_data[0, j] = item_feat[feat_id]
            feature_tensors[feat_id] = torch.from_numpy(batch_data)
        
        return feature_tensors

    def compute_flash_infonce_loss(self, seq_embs, pos_embs, neg_embs, pop_neg_embs, loss_mask, global_step, block_size=512):
        """
        Flash InfoNCE: åŸºäºFlash Attentionæ€æƒ³çš„å†…å­˜é«˜æ•ˆInfoNCEå®ç°
        """
        
        # 1. å½’ä¸€åŒ–åµŒå…¥
        seq_embs = F.normalize(seq_embs, dim=-1)
        pos_embs = F.normalize(pos_embs, dim=-1) 
        neg_embs = F.normalize(neg_embs, dim=-1)
        pop_neg_embs = F.normalize(pop_neg_embs, dim=-1)
        # 2. ç­›é€‰æœ‰æ•ˆä½ç½®
        valid_mask = loss_mask.bool()
        seq_embs_valid = seq_embs[valid_mask]  # [N, H]
        pos_embs_valid = pos_embs[valid_mask]  # [N, H] 
        
        if seq_embs_valid.size(0) == 0:
            return torch.tensor(0.0, device=seq_embs.device, requires_grad=True)
        
        # 3. è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆæ¸©åº¦ç¼©æ”¾å‰ï¼Œç”¨äºç»Ÿè®¡ï¼‰
        pos_similarities = F.cosine_similarity(seq_embs_valid, pos_embs_valid, dim=-1)  # [N] èŒƒå›´[-1,1]
        pos_logits = pos_similarities / self.temperature  # æ¸©åº¦ç¼©æ”¾åç”¨äºæŸå¤±è®¡ç®—
        
        # è®°å½•åŸå§‹ç›¸ä¼¼åº¦ï¼ˆèŒƒå›´[-1,1]ï¼‰
        self.writer.add_scalar('Model/flash_nce_pos_logits', pos_similarities.mean().item(), global_step)
        
        # 4. æ”¶é›†è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ç”¨äºç»Ÿè®¡
        _, _, hidden_size = neg_embs.shape
        neg_embs_flat = neg_embs.view(-1, hidden_size)  # [B*L, H] å·²ç»å½’ä¸€åŒ–è¿‡äº†
        neg_embs_flat = torch.cat([neg_embs_flat, pop_neg_embs.view(-1, hidden_size)], dim=0)
        total_neg_samples = neg_embs_flat.size(0)

        
        # åˆå§‹åŒ–åœ¨çº¿Softmaxç»Ÿè®¡é‡
        max_logits = pos_logits.clone()  # [N] 
        sum_exp = torch.exp(pos_logits - max_logits)  # [N]
        
        # ğŸ†• ç”¨äºç»Ÿè®¡çš„è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦æ”¶é›†
        neg_similarities_for_stats = []
        
        # 5. åˆ†å—å¤„ç†æ‰€æœ‰è´Ÿæ ·æœ¬
        for start_idx in range(0, total_neg_samples, block_size):
            end_idx = min(start_idx + block_size, total_neg_samples)
            
            # å½“å‰å—çš„è´Ÿæ ·æœ¬åµŒå…¥ï¼ˆå·²ç»å½’ä¸€åŒ–ï¼Œä¸è¦é‡å¤å½’ä¸€åŒ–ï¼‰
            neg_block = neg_embs_flat[start_idx:end_idx]  # [block_size, H]
            
            # è®¡ç®—å½“å‰å—çš„ç›¸ä¼¼åº¦ [N, block_size] èŒƒå›´[-1,1]
            block_similarities = torch.matmul(seq_embs_valid, neg_block.transpose(-1, -2))
            block_logits = block_similarities / self.temperature  # æ¸©åº¦ç¼©æ”¾å
            
            # ğŸ†• æ”¶é›†å°‘é‡è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ç”¨äºç»Ÿè®¡ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            if start_idx == 0:  # åªæ”¶é›†ç¬¬ä¸€ä¸ªå—çš„æ•°æ®ç”¨äºç»Ÿè®¡
                neg_similarities_for_stats = block_similarities.detach()
            
            # åœ¨çº¿æ›´æ–°Softmaxç»Ÿè®¡é‡
            block_max = torch.max(block_logits, dim=-1)[0]  # [N]
            new_max = torch.max(max_logits, block_max)  # [N]
            
            # é‡æ–°ç¼©æ”¾å¹¶æ›´æ–°
            sum_exp = sum_exp * torch.exp(max_logits - new_max)
            block_exp_sum = torch.sum(torch.exp(block_logits - new_max.unsqueeze(-1)), dim=-1)  # [N]
            sum_exp = sum_exp + block_exp_sum
            max_logits = new_max
        
        # 6. è®¡ç®—æœ€ç»ˆçš„InfoNCEæŸå¤±
        log_prob = pos_logits - max_logits - torch.log(sum_exp)
        loss = -log_prob.mean()
        
        # 7. æ­£ç¡®è®°å½•ç»Ÿè®¡ä¿¡æ¯
        with torch.no_grad():
            # è®°å½•è´Ÿæ ·æœ¬çš„çœŸå®ç›¸ä¼¼åº¦å¹³å‡å€¼ï¼ˆèŒƒå›´[-1,1]ï¼‰
            avg_neg_similarities = neg_similarities_for_stats.mean().item()
            self.writer.add_scalar('Model/flash_nce_neg_logits', avg_neg_similarities, global_step)
            self.writer.add_scalar('Model/flash_nce_max_logits', max_logits.mean().item(), global_step)
        
        return loss

    def compute_infonce_loss(self, seq_embs, pos_embs, neg_embs, pop_neg_embs, loss_mask, global_step):
        """
        InfoNCEæŸå¤±è®¡ç®—ï¼Œæ”¯æŒFlashå’ŒåŸå§‹ä¸¤ç§å®ç°
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Flashç‰ˆæœ¬
        if self.use_flash_infonce:
            return self.compute_flash_infonce_loss(
                seq_embs, pos_embs, neg_embs, pop_neg_embs, loss_mask, global_step, self.flash_block_size
            )
        
        # åŸå§‹å®ç°ä¿æŒä¸å˜
        hidden_size = neg_embs.size(-1)
        seq_embs = F.normalize(seq_embs, dim=-1)
        pos_embs = F.normalize(pos_embs, dim=-1)
        pos_logits = F.cosine_similarity(seq_embs, pos_embs, dim=-1).unsqueeze(-1)
        self.writer.add_scalar('Model/nce_pos_logits', pos_logits.mean().item(), global_step)

        neg_embs = F.normalize(neg_embs, dim=-1)
        pop_neg_embs = F.normalize(pop_neg_embs, dim=-1)
        neg_embedding_all = neg_embs.reshape(-1, hidden_size)
        neg_embedding_all = torch.cat([neg_embedding_all, pop_neg_embs.reshape(-1, hidden_size)], dim=0)
        neg_logits = torch.matmul(seq_embs, neg_embedding_all.transpose(-1, -2))
        self.writer.add_scalar('Model/nce_neg_logits', neg_logits.mean().item(), global_step)

        logits = torch.cat([pos_logits, neg_logits], dim=-1)
        logits = logits[loss_mask.bool()] / self.temperature
        labels = torch.zeros(logits.size(0), device=logits.device, dtype=torch.int64)
        loss = F.cross_entropy(logits, labels)

        return loss.mean()
    
    def forward_infonce(
        self, user_item, pos_seqs, neg_seqs, pop_neg_seqs, mask, next_mask, next_action_type, 
        seq_feature_tensors, pos_feature_tensors, neg_feature_tensors, pop_neg_feature_tensors, global_step
    ):
        """
        ä½¿ç”¨InfoNCEæŸå¤±çš„å‰å‘ä¼ æ’­å‡½æ•°

        Args:
            user_item: ç”¨æˆ·åºåˆ—ID
            pos_seqs: æ­£æ ·æœ¬åºåˆ—ID  
            neg_seqs: è´Ÿæ ·æœ¬åºåˆ—ID
            pop_neg_seqs: æµè¡Œåº¦è´Ÿæ ·æœ¬åºåˆ—ID
            mask: tokenç±»å‹æ©ç ï¼Œ1è¡¨ç¤ºitem tokenï¼Œ2è¡¨ç¤ºuser token
            next_mask: ä¸‹ä¸€ä¸ªtokenç±»å‹æ©ç ï¼Œ1è¡¨ç¤ºitem tokenï¼Œ2è¡¨ç¤ºuser token
            next_action_type: ä¸‹ä¸€ä¸ªtokenåŠ¨ä½œç±»å‹ï¼Œ0è¡¨ç¤ºæ›å…‰ï¼Œ1è¡¨ç¤ºç‚¹å‡»
            seq_feature_tensors: é¢„å¤„ç†å¥½çš„åºåˆ—ç‰¹å¾tensorå­—å…¸
            pos_feature_tensors: é¢„å¤„ç†å¥½çš„æ­£æ ·æœ¬ç‰¹å¾tensorå­—å…¸
            neg_feature_tensors: é¢„å¤„ç†å¥½çš„è´Ÿæ ·æœ¬ç‰¹å¾tensorå­—å…¸
            pop_neg_feature_tensors: é¢„å¤„ç†å¥½çš„æµè¡Œåº¦è´Ÿæ ·æœ¬ç‰¹å¾tensorå­—å…¸
        Returns:
            infonce_loss: InfoNCEæŸå¤±å€¼
        """
        # 1. è·å–åºåˆ—è¡¨ç¤º
        seq_embs = self.log2feats(user_item, mask, seq_feature_tensors)
        loss_mask = (next_mask == 1).to(self.dev)
        
        # 2. è·å–æ­£è´Ÿæ ·æœ¬åµŒå…¥
        pos_embs = self.feat2emb(pos_seqs, pos_feature_tensors, include_user=False)
        neg_embs = self.feat2emb(neg_seqs, neg_feature_tensors, include_user=False)
        pop_neg_embs = self.feat2emb(pop_neg_seqs, pop_neg_feature_tensors, include_user=False)
        # 3. è®¡ç®—InfoNCEæŸå¤±
        infonce_loss = self.compute_infonce_loss(seq_embs, pos_embs, neg_embs, pop_neg_embs, loss_mask, global_step)
        
        return infonce_loss


   