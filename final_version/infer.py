import argparse
import json
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import struct
from pathlib import Path
import hashlib
from torch.amp import autocast
from torch.utils.tensorboard import SummaryWriter
import gc
from contextlib import contextmanager

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from dataset import MyTestDataset, save_emb, OptimizedTestDataset
from model import BaselineModel

import yaml

@contextmanager
def memory_efficient_loading():
    """å†…å­˜é«˜æ•ˆåŠ è½½çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        # å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼
        torch.cuda.empty_cache()
        if hasattr(torch.backends.cudnn, 'benchmark'):
            original_benchmark = torch.backends.cudnn.benchmark
            torch.backends.cudnn.benchmark = False
        yield
    finally:
        # æ¢å¤è®¾ç½®
        if hasattr(torch.backends.cudnn, 'benchmark'):
            torch.backends.cudnn.benchmark = original_benchmark
        torch.cuda.empty_cache()

def load_model_progressively(usernum, itemnum, feat_statistics, feat_types, args, writer, ckpt_path):
    """
    æ¸è¿›å¼åŠ è½½æ¨¡å‹ï¼Œé¿å…å†…å­˜å³°å€¼è¿‡é«˜
    """
    print("ğŸš€ å¼€å§‹æ¸è¿›å¼æ¨¡å‹åŠ è½½...")
    
    with memory_efficient_loading():
        # 1. é¦–å…ˆåœ¨CPUä¸Šåˆ›å»ºæ¨¡å‹ç»“æ„ï¼ˆä¸å ç”¨GPUå†…å­˜ï¼‰
        print("  ğŸ“¦ åœ¨CPUä¸Šåˆ›å»ºæ¨¡å‹ç»“æ„...")
        model = BaselineModel(usernum, itemnum, feat_statistics, feat_types, args, writer)
        
        # 2. åŠ è½½checkpointåˆ°CPUå†…å­˜
        print("  ğŸ’¾ åŠ è½½checkpointåˆ°CPU...")
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        
        # 3. åˆ†ç»„åŠ è½½æƒé‡åˆ°GPU
        print("  ğŸ”„ åˆ†ç»„åŠ è½½æƒé‡åˆ°GPU...")
        
        # æ ¹æ®Tencent_Comp21_1çš„æ¨¡å‹ç»“æ„å®šä¹‰åˆ†ç»„ç­–ç•¥
        module_groups = {
            # åŸºç¡€åµŒå…¥å±‚
            'embeddings': ['item_emb', 'user_emb'],
            
            # ç¨€ç–ç‰¹å¾åµŒå…¥å±‚
            'sparse_embeddings': [f'sparse_emb.{k}' for k in model.sparse_emb.keys()],
            
            # å¤šæ¨¡æ€ç‰¹å¾å˜æ¢å±‚
            'transformations': [f'emb_transform.{k}' for k in model.emb_transform.keys()],
            
            # è¿ç»­ç‰¹å¾å˜æ¢å±‚
            'continual_transformations': [f'continual_transform.{k}' for k in model.continual_transform.keys()],
            
            # æ³¨æ„åŠ›å±‚ç›¸å…³
            'attention_layers': [name for name, _ in model.named_parameters() if 'attention_layers' in name],
            'attention_layernorms': [name for name, _ in model.named_parameters() if 'attention_layernorms' in name],
            
            # å‰å‘å±‚ç›¸å…³
            'forward_layers': [name for name, _ in model.named_parameters() if 'forward_layers' in name],
            'forward_layernorms': [name for name, _ in model.named_parameters() if 'forward_layernorms' in name],
            
            # SEåŒçº¿æ€§DNNå±‚
            'se_bilinear_dnn': [name for name, _ in model.named_parameters() if 'SeBilinearDNN' in name or 'userSeBilinearDNN' in name or 'itemSeBilinearDNN' in name],
            
            # æœ€åçš„LayerNorm
            'final_norm': ['last_layernorm'],
            
            # å…¶ä»–å‰©ä½™æ¨¡å—
            'others': []
        }
        
        # é€ç»„åŠ è½½
        for group_name, param_names in module_groups.items():
            if not param_names:
                continue
                
            print(f"    ğŸ“¥ åŠ è½½ {group_name} ç»„...")
            
            # åˆ›å»ºä¸´æ—¶å­—å…¸å­˜å‚¨å½“å‰ç»„çš„æƒé‡
            group_state_dict = {}
            for param_name in param_names:
                if param_name in checkpoint:
                    group_state_dict[param_name] = checkpoint[param_name]
                # å¤„ç†åµŒå¥—å‚æ•°å
                for full_name, param in checkpoint.items():
                    if any(pn in full_name for pn in param_names):
                        group_state_dict[full_name] = param
            
            # åŠ è½½å½“å‰ç»„åˆ°æ¨¡å‹
            if group_state_dict:
                model.load_state_dict(group_state_dict, strict=False)
                
            # å°†å½“å‰ç»„ç§»åˆ°GPU
            if group_name == 'embeddings':
                model.item_emb = model.item_emb.to(args.device)
                model.user_emb = model.user_emb.to(args.device)
            elif group_name == 'sparse_embeddings':
                model.sparse_emb = model.sparse_emb.to(args.device)
            elif group_name == 'transformations':
                model.emb_transform = model.emb_transform.to(args.device)
            elif group_name == 'continual_transformations':
                model.continual_transform = model.continual_transform.to(args.device)
            elif group_name == 'attention_layers':
                model.attention_layers = model.attention_layers.to(args.device)
            elif group_name == 'attention_layernorms':
                model.attention_layernorms = model.attention_layernorms.to(args.device)
            elif group_name == 'forward_layers':
                model.forward_layers = model.forward_layers.to(args.device)
            elif group_name == 'forward_layernorms':
                model.forward_layernorms = model.forward_layernorms.to(args.device)
            elif group_name == 'se_bilinear_dnn':
                model.userSeBilinearDNN = model.userSeBilinearDNN.to(args.device)
                model.itemSeBilinearDNN = model.itemSeBilinearDNN.to(args.device)
            elif group_name == 'final_norm':
                model.last_layernorm = model.last_layernorm.to(args.device)
            
            # æ¸…ç†CPUå†…å­˜
            del group_state_dict
            torch.cuda.empty_cache()
            gc.collect()
        
        # 4. æœ€åå°†æ•´ä¸ªæ¨¡å‹ç§»åˆ°GPUï¼ˆç¡®ä¿æ‰€æœ‰å‰©ä½™éƒ¨åˆ†éƒ½ç§»åŠ¨ï¼‰
        print("  ğŸ¯ å®Œæˆæ¨¡å‹åŠ è½½...")
        model = model.to(args.device)
        model.eval()
        
        # æ¸…ç†checkpointå†…å­˜
        del checkpoint
        torch.cuda.empty_cache()
        gc.collect()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return model

def get_ckpt_path():
    ckpt_path = os.environ.get("MODEL_OUTPUT_PATH")
    if ckpt_path is None:
        raise ValueError("MODEL_OUTPUT_PATH is not set")
    for item in os.listdir(ckpt_path):
        if item.endswith(".pt"):
            return os.path.join(ckpt_path, item)


def get_args():
    parser = argparse.ArgumentParser()

    # Train params
    parser.add_argument('--batch_size', default=16, type=int)
    parser.add_argument('--lr', default=0.0005, type=float)
    parser.add_argument('--maxlen', default=101, type=int)

    # Baseline Model construction
    parser.add_argument('--hidden_units', default=256, type=int)
    parser.add_argument('--num_blocks', default=8, type=int)
    parser.add_argument('--num_epochs', default=10, type=int)
    parser.add_argument('--num_heads', default=8, type=int)
    parser.add_argument('--dropout_rate', default=0.15, type=float)
    parser.add_argument('--l2_emb', default=0.0, type=float)
    parser.add_argument('--device', default='cuda', type=str)
    parser.add_argument('--inference_only', action='store_true')
    parser.add_argument('--state_dict_path', default=None, type=str)
    parser.add_argument('--norm_first', default=False, type=bool)
    parser.add_argument('--temperature', default=0.03, type=float)
    parser.add_argument('--infonce_weight', default=1, type=float)
    parser.add_argument('--weight_decay', default=0.005, type=float)

    # MMemb Feature ID
    parser.add_argument('--mm_emb_id', nargs='+', default=['81'], type=str, choices=[str(s) for s in range(81, 87)])

    parser.add_argument('--use_flash_infonce', action='store_true', default=True,
                       help='ä½¿ç”¨Flash InfoNCEä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼ŒåŸºäºFlash Attentionæ€æƒ³å®ç°åˆ†å—è®¡ç®—')
    parser.add_argument('--flash_block_size', default=1024, type=int,
                       help='Flash InfoNCEçš„åˆ†å—å¤§å°ï¼Œè¶Šå°æ˜¾å­˜å ç”¨è¶Šå°‘ä½†è®¡ç®—ç•¥æ…¢ï¼Œæ¨è256-1024')

    args = parser.parse_args()

    return args


def read_result_ids(file_path):
    with open(file_path, 'rb') as f:
        # Read the header (num_points_query and FLAGS_query_ann_top_k)
        num_points_query = struct.unpack('I', f.read(4))[0]  # uint32_t -> 4 bytes
        query_ann_top_k = struct.unpack('I', f.read(4))[0]  # uint32_t -> 4 bytes

        print(f"num_points_query: {num_points_query}, query_ann_top_k: {query_ann_top_k}")

        # Calculate how many result_ids there are (num_points_query * query_ann_top_k)
        num_result_ids = num_points_query * query_ann_top_k

        # Read result_ids (uint64_t, 8 bytes per value)
        result_ids = np.fromfile(f, dtype=np.uint64, count=num_result_ids)

        return result_ids.reshape((num_points_query, query_ann_top_k))


def _stable_hash_to_int(text: str) -> int:
    """å°†å­—ç¬¦ä¸²ç¨³å®šæ˜ å°„ä¸ºéè´Ÿæ•´æ•°ï¼ˆä¸Pythonè¿›ç¨‹æ— å…³ï¼‰ã€‚"""
    if not isinstance(text, str):
        text = str(text)
    h = hashlib.md5(text.encode('utf-8')).digest()
    return int.from_bytes(h[:8], 'little', signed=False)


def process_cold_start_feat(feat, feat_types, feat_statistics):
    """
    å†·å¯åŠ¨ç‰¹å¾ä¼˜åŒ–ï¼š
    - ç¨€ç–/å¤šå€¼ç¨€ç–ç‰¹å¾ï¼šæœªè§è¿‡çš„å­—ç¬¦ä¸²ä½¿ç”¨å“ˆå¸Œåˆ†æ¡¶æ˜ å°„åˆ°åˆæ³•IDèŒƒå›´ï¼ˆé¿å¼€padding=0ï¼‰
    - è¿ç»­ç‰¹å¾ï¼šæ— æ³•è§£ææ—¶å›é€€ä¸º0.0
    """
    processed_feat = {}
    if feat is None:
        return processed_feat

    sparse_keys = set(feat_types.get('item_sparse', [])) | set(feat_types.get('user_sparse', []))
    array_keys = set(feat_types.get('item_array', [])) | set(feat_types.get('user_array', []))
    continual_keys = set(feat_types.get('item_continual', [])) | set(feat_types.get('user_continual', []))

    for feat_id, feat_value in feat.items():
        if feat_id in sparse_keys:
            if isinstance(feat_value, str):
                vocab = max(1, int(feat_statistics.get(feat_id, 1)))
                mapped = 1 + (_stable_hash_to_int(feat_value) % vocab)
                processed_feat[feat_id] = mapped
            else:
                processed_feat[feat_id] = int(feat_value)
        elif feat_id in array_keys:
            if isinstance(feat_value, list):
                vocab = max(1, int(feat_statistics.get(feat_id, 1)))
                mapped_list = []
                for v in feat_value:
                    if isinstance(v, str):
                        mapped_list.append(1 + (_stable_hash_to_int(v) % vocab))
                    else:
                        mapped_list.append(int(v))
                processed_feat[feat_id] = mapped_list
            elif isinstance(feat_value, str):
                vocab = max(1, int(feat_statistics.get(feat_id, 1)))
                processed_feat[feat_id] = [1 + (_stable_hash_to_int(feat_value) % vocab)]
            else:
                processed_feat[feat_id] = [int(feat_value)]
        elif feat_id in continual_keys:
            if isinstance(feat_value, str):
                processed_feat[feat_id] = 0.0
            else:
                try:
                    processed_feat[feat_id] = float(feat_value)
                except Exception:
                    processed_feat[feat_id] = 0.0
        else:
            # å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚å¤šæ¨¡æ€embç”±å¤–å±‚å¡«å……ï¼‰ä¿æŒåŸæ ·
            processed_feat[feat_id] = feat_value
    return processed_feat


def get_candidate_emb(indexer, feat_types, feat_default_value, mm_emb_dict, feat_statistics, model):
    """
    ç”Ÿäº§å€™é€‰åº“itemçš„idå’Œembedding

    Args:
        indexer: ç´¢å¼•å­—å…¸
        feat_types: ç‰¹å¾ç±»å‹ï¼Œåˆ†ä¸ºuserå’Œitemçš„sparse, array, emb, continualç±»å‹
        feature_default_value: ç‰¹å¾ç¼ºçœå€¼
        mm_emb_dict: å¤šæ¨¡æ€ç‰¹å¾å­—å…¸
        model: æ¨¡å‹
    Returns:
        retrieve_id2creative_id: ç´¢å¼•id->creative_idçš„dict
    """
    EMB_SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
    candidate_path = Path(os.environ.get('EVAL_DATA_PATH'), 'predict_set.jsonl')
    item_ids, creative_ids, retrieval_ids, features = [], [], [], []
    retrieve_id2creative_id = {}

    # é¢„è®¡ç®—å„å¤šæ¨¡æ€ç‰¹å¾çš„å‡å€¼å‘é‡ï¼Œä½œä¸ºå†·å¯åŠ¨å›é€€ï¼ˆæ¯”å…¨é›¶æ›´ç¨³ï¼‰
    EMB_DEFAULTS = {}
    for emb_k, emb_map in mm_emb_dict.items():
        try:
            if len(emb_map) > 0:
                EMB_DEFAULTS[emb_k] = np.mean(np.stack(list(emb_map.values()), axis=0), axis=0).astype(np.float32)
        except Exception:
            pass

    with open(candidate_path, 'r') as f:
        for line in f:
            line = json.loads(line)
            # è¯»å–itemç‰¹å¾ï¼Œå¹¶è¡¥å……ç¼ºå¤±å€¼
            feature = line['features']
            creative_id = line['creative_id']
            retrieval_id = line['retrieval_id']
            item_id = indexer[creative_id] if creative_id in indexer else 0
            missing_fields = set(
                feat_types['item_sparse'] + feat_types['item_array'] + feat_types['item_continual']
            ) - set(feature.keys())
            feature = process_cold_start_feat(feature, feat_types, feat_statistics)
            for feat_id in missing_fields:
                feature[feat_id] = feat_default_value[feat_id]
            for feat_id in feat_types['item_emb']:
                if creative_id in mm_emb_dict[feat_id]:
                    feature[feat_id] = mm_emb_dict[feat_id][creative_id]
                else:
                    if feat_id in EMB_DEFAULTS:
                        feature[feat_id] = EMB_DEFAULTS[feat_id]
                    else:
                        feature[feat_id] = np.zeros(EMB_SHAPE_DICT[feat_id], dtype=np.float32)

            item_ids.append(item_id)
            creative_ids.append(creative_id)
            retrieval_ids.append(retrieval_id)
            features.append(feature)
            retrieve_id2creative_id[retrieval_id] = creative_id

    # ä¿å­˜å€™é€‰åº“çš„embeddingå’Œsid
    model.save_item_emb(item_ids, retrieval_ids, features, os.environ.get('EVAL_RESULT_PATH'))
    with open(Path(os.environ.get('EVAL_RESULT_PATH'), "retrive_id2creative_id.json"), "w") as f:
        json.dump(retrieve_id2creative_id, f)
    return retrieve_id2creative_id

def infer():
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    args = get_args()
    data_path = os.environ.get('EVAL_DATA_PATH')
    writer = SummaryWriter(os.environ.get('TRAIN_TF_EVENTS_PATH'))
    user_cache = os.environ.get('USER_CACHE_PATH')
    
    # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("ğŸš€ Using OptimizedTestDataset for faster inference...")
    test_dataset = OptimizedTestDataset(data_path, user_cache, args)
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=4,  # å¯ç”¨å¤šè¿›ç¨‹
        collate_fn=test_dataset.collate_fn,
        pin_memory=torch.cuda.is_available(),  # GPUå†…å­˜å›ºå®š
        persistent_workers=True,  # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=2  # é¢„è¯»å–å› å­
    )
    usernum, itemnum = test_dataset.usernum, test_dataset.itemnum
    feat_statistics, feat_types = test_dataset.feat_statistics, test_dataset.feature_types
    model = BaselineModel(usernum, itemnum, feat_statistics, feat_types, args, writer).to(args.device)
    model.eval()

    ckpt_path = get_ckpt_path()
    model = load_model_progressively(usernum, itemnum, feat_statistics, feat_types, args, writer, ckpt_path)

    all_embs = []
    user_list = []

    # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
    import time
    start_time = time.time()
    temp_time = start_time
    print(f"Inference started at {time.strftime('%Y-%m-%d %H:%M:%S')}")
    for step, batch in enumerate(test_loader):
        seq, token_type, seq_feat, user_id = batch
        seq = seq.to(args.device)
        
        # æ·»åŠ æ˜¾å­˜æ¸…ç†
        if step % 100 == 0:  # æ¯100æ­¥æ¸…ç†ä¸€æ¬¡
            torch.cuda.empty_cache()
        
        with torch.no_grad():
            with autocast(device_type="cuda", dtype=torch.bfloat16):
                logits = model.predict(seq, seq_feat, token_type)
        
        # ç«‹å³å°†ç»“æœç§»åˆ°CPU
        for i in range(logits.shape[0]):
            emb = logits[i].unsqueeze(0).detach().cpu().numpy().astype(np.float32)
            all_embs.append(emb)
        
        # æ¸…ç†GPUä¸Šçš„ä¸­é—´ç»“æœ
        del logits
        torch.cuda.empty_cache()
        
        user_list += user_id
        if step % 1000 == 0:
            print(f"Inference step {step} time: {time.time() - temp_time:.2f}s")
            temp_time = time.time()

    # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
    inference_time = time.time() - start_time
    total_samples = len(user_list)
    samples_per_second = total_samples / inference_time if inference_time > 0 else 0

    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats = test_dataset.get_cache_stats()
    
    print(f"\nğŸ“Š Inference Performance Stats:")
    print(f"   â€¢ Total samples: {total_samples}")
    print(f"   â€¢ Inference time: {inference_time:.2f}s") 
    print(f"   â€¢ Throughput: {samples_per_second:.1f} samples/sec")
    print(f"   â€¢ Cache hit rate: {cache_stats['hit_rate']}")
    print(f"   â€¢ Cache size: {cache_stats['cache_size']}")
    print(f"   â€¢ DataLoader workers: {test_loader.num_workers}")
    print(f"   â€¢ Batch size: {args.batch_size}")

    # ç”Ÿæˆå€™é€‰åº“çš„embedding ä»¥åŠ idæ–‡ä»¶
    retrieve_id2creative_id = get_candidate_emb(
        test_dataset.indexer['i'],
        test_dataset.feature_types,
        test_dataset.feature_default_value,
        test_dataset.mm_emb_dict,
        test_dataset.feat_statistics,
        model,
    )
    all_embs = np.concatenate(all_embs, axis=0)
    # ä¿å­˜queryæ–‡ä»¶
    save_emb(all_embs, Path(os.environ.get('EVAL_RESULT_PATH'), 'query.fbin'))

    del model, test_loader, test_dataset, all_embs
    writer.close()
    del writer
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    
    # ANN æ£€ç´¢ - ä½¿ç”¨PyTorch CUDAå®ç°æ›¿ä»£C++ç‰ˆæœ¬
    ann_cmd = [
        "python", str(Path(__file__).parent / "torch_ann.py"),
        "--dataset_vector_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "embedding.fbin")),
        "--dataset_id_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "id.u64bin")),
        "--query_vector_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "query.fbin")),
        "--result_id_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "id100.u64bin")),
        "--query_ann_top_k", "10",
        "--faiss_metric_type", "0",  # ä½¿ç”¨å†…ç§¯/ä½™å¼¦ç›¸ä¼¼åº¦
        "--batch_size", "1000"  # æ‰¹å¤„ç†å¤§å°ï¼Œå¯æ ¹æ®GPUå†…å­˜è°ƒæ•´
    ]
    import subprocess
    result = subprocess.run(ann_cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"ANN search failed with error: {result.stderr}")
        raise RuntimeError(f"ANN search failed: {result.stderr}")
    else:
        print(f"ANN search completed successfully: {result.stdout}")

    # å–å‡ºtop-k
    top10s_retrieved = read_result_ids(Path(os.environ.get("EVAL_RESULT_PATH"), "id100.u64bin"))
    top10s_untrimmed = []
    for top10 in tqdm(top10s_retrieved):
        for item in top10:
            top10s_untrimmed.append(retrieve_id2creative_id.get(int(item), 0))

    top10s = [top10s_untrimmed[i : i + 10] for i in range(0, len(top10s_untrimmed), 10)]

    return top10s, user_list

# def infer():
#     os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
#     args = get_args()
#     data_path = os.environ.get('EVAL_DATA_PATH')
#     writer = SummaryWriter(os.environ.get('TRAIN_TF_EVENTS_PATH'))
#     user_cache = os.environ.get('USER_CACHE_PATH')
    
#     # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
#     print("ğŸš€ Using OptimizedTestDataset for faster inference...")
#     test_dataset = OptimizedTestDataset(data_path, user_cache, args)
#     test_loader = DataLoader(
#         test_dataset, 
#         batch_size=args.batch_size, 
#         shuffle=False, 
#         num_workers=4,  # å¯ç”¨å¤šè¿›ç¨‹
#         collate_fn=test_dataset.collate_fn,
#         pin_memory=torch.cuda.is_available(),  # GPUå†…å­˜å›ºå®š
#         persistent_workers=True,  # ä¿æŒworkerè¿›ç¨‹
#         prefetch_factor=2  # é¢„è¯»å–å› å­
#     )
#     usernum, itemnum = test_dataset.usernum, test_dataset.itemnum
#     feat_statistics, feat_types = test_dataset.feat_statistics, test_dataset.feature_types
#     model = BaselineModel(usernum, itemnum, feat_statistics, feat_types, args, writer).to(args.device)
#     model.eval()

#     ckpt_path = get_ckpt_path()
#     ckpt = torch.load(ckpt_path, map_location=torch.device(args.device))


#     model.load_state_dict(ckpt)  # âœ… ç°åœ¨è‚¯å®šèƒ½åŠ è½½ï¼

#     all_embs = []
#     user_list = []

#     # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
#     import time
#     start_time = time.time()
#     temp_time = start_time
#     print(f"Inference started at {time.strftime('%Y-%m-%d %H:%M:%S')}")
#     for step, batch in enumerate(test_loader):
#         seq, token_type, seq_feat, user_id = batch
#         seq = seq.to(args.device)
        
#         # æ·»åŠ æ˜¾å­˜æ¸…ç†
#         if step % 100 == 0:  # æ¯100æ­¥æ¸…ç†ä¸€æ¬¡
#             torch.cuda.empty_cache()
        
#         with torch.no_grad():
#             with autocast(device_type="cuda", dtype=torch.bfloat16):
#                 logits = model.predict(seq, seq_feat, token_type)
        
#         # ç«‹å³å°†ç»“æœç§»åˆ°CPU
#         for i in range(logits.shape[0]):
#             emb = logits[i].unsqueeze(0).detach().cpu().numpy().astype(np.float32)
#             all_embs.append(emb)
        
#         # æ¸…ç†GPUä¸Šçš„ä¸­é—´ç»“æœ
#         del logits
#         torch.cuda.empty_cache()
        
#         user_list += user_id
#         if step % 1000 == 0:
#             print(f"Inference step {step} time: {time.time() - temp_time:.2f}s")
#             temp_time = time.time()

#     # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
#     inference_time = time.time() - start_time
#     total_samples = len(user_list)
#     samples_per_second = total_samples / inference_time if inference_time > 0 else 0

#     # è·å–ç¼“å­˜ç»Ÿè®¡
#     cache_stats = test_dataset.get_cache_stats()
    
#     print(f"\nğŸ“Š Inference Performance Stats:")
#     print(f"   â€¢ Total samples: {total_samples}")
#     print(f"   â€¢ Inference time: {inference_time:.2f}s") 
#     print(f"   â€¢ Throughput: {samples_per_second:.1f} samples/sec")
#     print(f"   â€¢ Cache hit rate: {cache_stats['hit_rate']}")
#     print(f"   â€¢ Cache size: {cache_stats['cache_size']}")
#     print(f"   â€¢ DataLoader workers: {test_loader.num_workers}")
#     print(f"   â€¢ Batch size: {args.batch_size}")

#     # ç”Ÿæˆå€™é€‰åº“çš„embedding ä»¥åŠ idæ–‡ä»¶
#     retrieve_id2creative_id = get_candidate_emb(
#         test_dataset.indexer['i'],
#         test_dataset.feature_types,
#         test_dataset.feature_default_value,
#         test_dataset.mm_emb_dict,
#         test_dataset.feat_statistics,
#         model,
#     )
#     all_embs = np.concatenate(all_embs, axis=0)
#     # ä¿å­˜queryæ–‡ä»¶
#     save_emb(all_embs, Path(os.environ.get('EVAL_RESULT_PATH'), 'query.fbin'))

#     del model, test_loader, test_dataset, all_embs
#     writer.close()
#     del writer
#     import gc
#     gc.collect()
#     torch.cuda.empty_cache()
#     torch.cuda.synchronize()
    
#     # ANN æ£€ç´¢ - ä½¿ç”¨PyTorch CUDAå®ç°æ›¿ä»£C++ç‰ˆæœ¬
#     ann_cmd = [
#         "python", str(Path(__file__).parent / "torch_ann.py"),
#         "--dataset_vector_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "embedding.fbin")),
#         "--dataset_id_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "id.u64bin")),
#         "--query_vector_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "query.fbin")),
#         "--result_id_file_path", str(Path(os.environ.get("EVAL_RESULT_PATH"), "id100.u64bin")),
#         "--query_ann_top_k", "10",
#         "--faiss_metric_type", "0",  # ä½¿ç”¨å†…ç§¯/ä½™å¼¦ç›¸ä¼¼åº¦
#         "--batch_size", "1000"  # æ‰¹å¤„ç†å¤§å°ï¼Œå¯æ ¹æ®GPUå†…å­˜è°ƒæ•´
#     ]
#     import subprocess
#     result = subprocess.run(ann_cmd, capture_output=True, text=True)
#     if result.returncode != 0:
#         print(f"ANN search failed with error: {result.stderr}")
#         raise RuntimeError(f"ANN search failed: {result.stderr}")
#     else:
#         print(f"ANN search completed successfully: {result.stdout}")

#     # å–å‡ºtop-k
#     top10s_retrieved = read_result_ids(Path(os.environ.get("EVAL_RESULT_PATH"), "id100.u64bin"))
#     top10s_untrimmed = []
#     for top10 in tqdm(top10s_retrieved):
#         for item in top10:
#             top10s_untrimmed.append(retrieve_id2creative_id.get(int(item), 0))

#     top10s = [top10s_untrimmed[i : i + 10] for i in range(0, len(top10s_untrimmed), 10)]

#     return top10s, user_list